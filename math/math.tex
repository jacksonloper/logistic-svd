\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}

\begin{document}

\title{Notes}

\author{Jackson Loper}

\date{January 2019}

\maketitle

\section{Objective}

For any fixed matrix $X$ with entries $X_{cg}\in \{0,1\}$ and any $n\geq1$, let
\begin{align*}
L &= \sum_{c,g} \left((X_{c,g}-.5)\left(\sum_k^n Z_{ck} \alpha_{gk}\right) - \log 2 \cosh \frac{1}{2}\sum_k^n Z_{ck} \alpha_{gk}\right)
\end{align*}
We here consider the problem of maximizing $L$ with respect to $\alpha,z$.  

We additionally consider the case that we would like to maximize a regularized objective.  Specifically, let 
\begin{align*}
R^\alpha &= \sum_g-\frac{1}{2}\alpha_g ^T D^\alpha_g \alpha_g + \alpha_g^Td^\alpha_g\\
R^z &= \sum_c-\frac{1}{2}z_c ^T D^z_c z_c + z_c^Td^z_c
\end{align*}
where for each $g$ we have $D^\alpha_g$ is an $n\times n$ square matrix, $d^\alpha$ is a $n$-vector, and likewise for $D^z,d^z$.  We can incorporate these regularizations by trying to maximize $L+R^\alpha+R^z$ instead.

\section{What this code provides}

\begin{enumerate}
    \item $z,\alpha\gets \mathtt{logistic\_svd.numpy\_version.initialize}(X)$.  Given $X$, uses SVD to give a reasonable initial estimate for $z,\alpha$.
    \item $\alpha'\gets \mathtt{logistic\_svd.numpy\_version.update\_alpha}(X,z,\alpha,D^\alpha,d^\alpha)$.  Given $X,D^\alpha,d^\alpha$ and an initial guess $z,\alpha$, this function calculates an improved estimate for $\alpha'$, i.e. $L(z,\alpha)+R^\alpha(\alpha) \leq L(z,\alpha')+R^{\alpha}(\alpha')$.  Note that, by the symmetry of this problem, this can be used to update $z$ as well.  
    \item $\alpha'\gets \mathtt{logistic\_svd.torch\_version.update\_alpha}(X,z,\alpha,D^\alpha,d^\alpha)$.  Same as above, but taking torch tensors as input instead of numpy arrays.
    \item $L \gets \mathtt{logistic\_svd.numpy\_version.logistic\_likelihood}(X,z,\alpha)$.  Calculates the (unregularized) objective.
    \item $L \gets \mathtt{logistic\_svd.torch\_version.logistic\_likelihood}(X,z,\alpha)$.  Same but for torch.
    \item $L \gets \mathtt{logistic\_svd.numpy\_version.quadratic}(z,D^z,d^z)$.  Calculates the regularization.
    \item $L \gets \mathtt{logistic\_svd.torch\_version.quadratic}(z,D^z,d^z)$.  Same but for torch.
\end{enumerate}

\section{How the updates work: minorization}

Observe that for any initial condition, $\tilde Z,\tilde \alpha$, we may obtain a simple minorizaton for this problem.  Indeed, let
\begin{align*}
M_{cg}=M_{cg}(\tilde Z,\tilde \alpha) &=\frac{\tanh \left(\frac{1}{2}\sum_k \tilde Z_{ck} \tilde \alpha_{gk}\right)}{2\sum_k \tilde Z_{ck} \tilde \alpha_{gk}}\\
\kappa_{cg} = \kappa_{cg}(\tilde Z,\tilde \alpha) &=  \frac{1}{2}M_{cg}\left(\sum_k \tilde Z_{ck} \tilde \alpha_{gk}\right)^2 - \log 2 \cosh \frac{1}{2}\sum_k \tilde Z_{ck} \tilde \alpha_{gk}\\
\tilde L_{M,k}(Z,\alpha) &= \sum_{c,g} \left(X_{c,g}\left(\sum_k Z_{ck} \alpha_{gk}\right) + \kappa_{cg} - \frac{1}{2}M_{cg}\left(\sum_k Z_{ck} \alpha_{gk}\right)^2 \right)
\end{align*}
Then observe that
\[
\tilde L_{M,k}(\tilde Z,\tilde \alpha) = L(\tilde Z,\tilde \alpha)
\]
Furthermore, it is well-known that
\[
\tilde L_{M,k}(Z,\alpha) \leq L(Z,\alpha) \qquad \forall Z,\alpha
\]
Thus $\tilde L$ is a so-called ``minorizer'' for $L$ from the initial condition $\tilde Z,\tilde \alpha$.  We can therefore be guaranteed that if we can find $Z,\alpha$ that improves $\tilde L$, it will also improve our value of $L$.  That is, if we can find $Z,\alpha$ such that $\tilde L_{M,k}(Z,\alpha)>\tilde L_{M,k}(\tilde Z,\tilde \alpha)$, then we will also have $L(Z,\alpha)>L(\tilde Z,\tilde \alpha)$.  This suggests the following iterative process:

\begin{enumerate}
    \item Start with some initial condition $\tilde Z,\tilde \alpha$.
    \item Calculate $M(\tilde Z,\tilde \alpha),k(\tilde Z,\tilde \alpha)$
    \item Find $Z,\alpha$ such that $\tilde L_{M,k}(Z,\alpha)>\tilde L_{M,k}(\tilde Z,\tilde \alpha)$
    \item Set $\tilde Z \gets Z$, $\tilde \alpha \gets \alpha$, go to step 2.
\end{enumerate}

To enact this procedure, the key difficulty is step 3.  That is, we need to be able to make progress on the surrogate problem $\tilde L$.  It is to this problem we now turn our attention.

\section{Progress on the surrogate problem $\tilde L$}

Here we consider the problem of optimizing 
\[
\tilde L_{M,k}(Z,\alpha) = \sum_{c,g} \left(X_{c,g}\left(\sum_k Z_{ck} \alpha_{gk}\right) - \frac{1}{2}M_{cg}\left(\sum_k Z_{ck} \alpha_{gk}\right)^2 \right)
\]
Note we have dropped the $\kappa$s that appeared in the previous section, since it is constant with respect to our objects of interest.  

This problem can be optimized via coordinate ascent, alternating between $Z$ and $\alpha$.  For example, let us consider only the case that we fix $\alpha$ and try to optimize $Z$.  Note that with $\alpha$ fixed the problem is now separable over the $c$s.  In particular, dropping constants, we see that for each $c$ separately we need to optimize a problem of the form
\[
f_c(z_c) = \sum_{g} \left(X_{c,g}\left(\sum_k Z_{ck} \alpha_{gk}\right) - \frac{1}{2}M_{cg}\left(\sum_k Z_{ck} \alpha_{gk}\right)^2 \right)
\]
Take derivatives:
\[
\frac{\partial}{\partial z_{ck}}f_c(z_c) = \sum_{g} X_{c,g}\alpha_{gk} - M_{cg}\alpha_{gk}\left(\sum_{k'} Z_{ck'} \alpha_{gk'}\right) 
\]
Setting equal to zero, we see that the optimal $\alpha_g$ will be achieved by taking
\begin{align*}
\Gamma_{k,k'} &= \sum_g M_{cg}\alpha_{gk}\alpha_{gk'}\\
z_c^* &= \Gamma^{-1} \alpha^T X_c
\end{align*}

We can do the same kind of update for $\alpha$.

\section{Initialization}

If we initialize our problem with $Z=\alpha=0$, our first minorization is given by taking $M_{cg}=\lim_{\epsilon\rightarrow 0}\tanh(\epsilon/2)/(2\epsilon) = .25$.  This leads to the surrogate problem
\[
\tilde L_{M,k}(Z,\alpha) = \sum_{c,g} \left(X_{c,g}\left(\sum_k Z_{ck} \alpha_{gk}\right) - \frac{1}{8}\left(\sum_k Z_{ck} \alpha_{gk}\right)^2 \right)
\]
It is easy to see that this problem is solved by taking $Z,\alpha$ as the first left and right singular vectors of $4(X-.5)$, each multiplied by the square root of the corresponding singular value.  This gives a good initialization.

\section{Regularization}

Introducing per-$c$ and per-$g$ quadratic regularizations is straightforward.  WLOG, let us consider updating $z_c$.  After the minorization recall that the problem has become separable.  The objective for a particular $c$, with regularization, is then
\[
f_c(z_c) = -\frac{1}{2} z_c^T D z_c + d^Tz_c + \sum_{g} \left(X_{c,g}\left(\sum_k Z_{ck} \alpha_{gk}\right) - \frac{1}{2}M_{cg}\left(\sum_k Z_{ck} \alpha_{gk}\right)^2 \right)
\]
It is straightforward to see that this leads to the updates
\begin{align*}
\Gamma_{k,k'} &= \sum_g M_{cg}\alpha_{gk}\alpha_{gk'}\\
z_c^* &= (\Gamma+D)^{-1} (\alpha^T X_c+d)
\end{align*}


\end{document}